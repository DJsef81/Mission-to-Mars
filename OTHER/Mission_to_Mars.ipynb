{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.3.3 Scrape Mars Data: The News\n",
    "\n",
    "# The script we're building is designed to scrape the most recent data—that means that each time we run the script, \n",
    "# we'll pull the newest data available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Splinter and BeautifulSoup\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "#add this because next cell says chromedrivermanager is not defined\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Current google-chrome version is 87.0.4280\n",
      "[WDM] - Get LATEST driver version for 87.0.4280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - Driver [/Users/josefanolin/.wdm/drivers/chromedriver/mac64/87.0.4280.88/chromedriver] found in cache\n"
     ]
    }
   ],
   "source": [
    "# Set the executable path and initialize the chrome browser in splinter (code from the reading, uses a manual path but \n",
    "# when i attempted to use the downloadabele chromedriver and installed it into the path, my mac said it didn't trust it.\n",
    "\n",
    "# executable_path = {'executable_path': '/usr/local/bin/chromedriver'}\n",
    "# browser = Browser('chrome', **executable_path)\n",
    "\n",
    "# Set the executable path and initialize chrome browser in splinter (alternate way)\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser('chrome', **executable_path, headless=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  we'll assign the url and instruct the browser to visit it.\n",
    "\n",
    "# Visit the mars nasa news site\n",
    "url = 'https://mars.nasa.gov/news/'\n",
    "browser.visit(url)\n",
    "# Optional delay for loading the page\n",
    "browser.is_element_present_by_css(\"ul.item_list li.slide\", wait_time=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The line, browser.is_element_present_by_css(\"ul.item_list li.slide\", wait_time=1), does two things.\n",
    "\n",
    "# One is that we're searching for elements with a specific combination of tag (ul and li) and attribute (item_list \n",
    "# and slide, respectively). For example, ul.item_list would be found in HTML as <ul class=”item_list”>.\n",
    "\n",
    "# Secondly, we're also telling our browser to wait one second before searching for components. \n",
    "# The optional delay is useful because sometimes dynamic pages take a little while to load, especially if they are \n",
    "# image-heavy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the HTML parser:\n",
    "\n",
    "html = browser.html\n",
    "news_soup = soup(html, 'html.parser')\n",
    "slide_elem = news_soup.select_one('ul.item_list li.slide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notice how we've assigned slide_elem as the variable to look for the <ul /> tag and its descendent (the other tags\n",
    "# within the <ul /> element), the <li /> tags? \n",
    "\n",
    "# This is our parent element. \n",
    "# This means that this element holds all of the other elements within it, and we'll reference it when we want to \n",
    "# filter search results even further. \n",
    "\n",
    "# The . is used for selecting classes, such as item_list, so the code 'ul.item_list li.slide' pinpoints the <li /> tag\n",
    "# with the class of slide and the <ul /> tag with a class of item_list. \n",
    "\n",
    "# CSS works from right to left, such as returning the last item on the list instead of the first. \n",
    "\n",
    "# Because of this, when using select_one, the first matching element returned will be a <li /> element with a class of \n",
    "# slide and all nested elements within it.\n",
    "\n",
    "# After opening the page in a new browser, right-click to inspect and activate your DevTools. \n",
    "# Then search for the HTML components you'll use to identify the title and paragraph you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which HTML attribute will we use to scrape the article’s title?\n",
    "\n",
    "# We’re looking for a <div /> with a class of “content_title.”\n",
    "\n",
    "# class = “content_title”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<div class=\"content_title\"><a href=\"/news/8822/a-martian-roundtrip-nasas-perseverance-rover-sample-tubes/\" target=\"_self\">A Martian Roundtrip: NASA's Perseverance Rover Sample Tubes</a></div>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll want to assign the title and summary text to variables we'll reference later. \n",
    "# let's begin our scraping\n",
    "\n",
    "# In this line of code, we'll chain .find onto our previously assigned variable, slide_elem. \n",
    "# When we do this, we're saying, \"This variable holds a ton of information, so look inside of that information to find \n",
    "# this specific data.\" \n",
    "\n",
    "# The data we're looking for is the content title, which we've specified by saying, \"The specific data is in a <div /> \n",
    "# with a class of 'content_title'.\"\n",
    "\n",
    "#  run this cell. The output should be the HTML containing the content title and anything else nested inside of \n",
    "# that <div />.\n",
    "\n",
    "slide_elem.find(\"div\", class_='content_title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"A Martian Roundtrip: NASA's Perseverance Rover Sample Tubes\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to get just the text, and the extra HTML stuff isn't necessary.\n",
    "\n",
    "# Use the parent element to find the first `a` tag and save it as `news_title`\n",
    "# the get_text(). when used is chained onto .find(), so only the text of the element is returned. \n",
    "news_title = slide_elem.find(\"div\", class_='content_title').get_text()\n",
    "news_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Earlier, we identified the parent element and created a variable to hold it. With this new code, we’re searching \n",
    "# within that element for the title. We’re also stripping the additional HTML attributes and tags with the use of \n",
    "# .get_text().\n",
    "\n",
    "# Once executed, the result is the most recent title published on the website. \n",
    "# When the website is updated and a new article is posted, when our code is run again, it will return that article \n",
    "# instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have the title we want, and that's a great start. Next we need to add the summary text. \n",
    "\n",
    "# slide_elem.find(“div”, class_=‘content_title’).get_text() is our previous code. \n",
    "# We’ll need to change the class to “article_teaser_body.”\n",
    "\n",
    "# Before we can update our code, we'll need to use our DevTools to make sure we're scraping the right tag and class. \n",
    "# We know that \"article_teaser_body\" is the right class name, but when we search for it, there is more than one result. \n",
    "# What now?\n",
    "\n",
    "# We want to pull the first one on the list, not a specific one, so more than 40 results is fine. \n",
    "# In this case, if our scraping code is too specific, we'd pull only that article summary instead of the most recent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are two methods used to find tags and attributes with BeautifulSoup:\n",
    "\n",
    "# .find() is used when we want only the first class and attribute we've specified.\n",
    "# .find_all() is used when we want to retrieve all of the tags and attributes.\n",
    "\n",
    "# For example, if we were to use .find_all() instead of .find() when pulling the summary, we would retrieve all of \n",
    "# the summaries on the page instead of just the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Marvels of engineering, the rover's sample tubes must be tough enough to safely bring Red Planet samples on the long journey back to Earth in immaculate condition. \""
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the parent element to find the paragraph text\n",
    "news_p = slide_elem.find('div', class_=\"article_teaser_body\").get_text()\n",
    "news_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10.3.4 Scrape Mars Data: Featured Image\n",
    "\n",
    "# Robin's next step scraping code will be to gather the featured images from NASA's Space Images webpage. \n",
    "# In your Jupyter notebook, use markdown to separate the article scraping from the image scraping.\n",
    "\n",
    "# In the next empty cell, type ### Featured Images and change the format of the code cell to \"Markdown.\"\n",
    "# The cell below this one is where we'll begin our scraping. First, let's check out the webpage.\n",
    "\n",
    "# The first image that pops up on the webpage is the featured image. \n",
    "# Robin wants the full-size version of this image, so we know we'll want Splinter to click the \"Full Image\" button. \n",
    "# From there, the page directs us to a slideshow. \n",
    "\n",
    "# It's a little closer to getting the full-size feature image, but we aren't quite there yet.\n",
    "\n",
    "# Click the \"More Info\" button on the page. Click image again to get to full size image. \n",
    "# Begin code ready to automate all of the clicks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Featured Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visit URL\n",
    "# A new automated browser should open to the featured images webpage.\n",
    "\n",
    "url = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we want to click the \"Full Image\" button. This button will direct our browser to an image slideshow. \n",
    "# Let's take a look at the button's HTML tags and attributes with the DevTools.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"button fancybox\" data-description=\"Scientists produced new global maps of Jupiter using the Wide Field Camera 3 on NASA's Hubble Space Telescope. One color map is shown here, projected onto a globe and as a flat image.\" data-fancybox-group=\"images\" data-fancybox-href=\"/spaceimages/images/mediumsize/PIA19643_ip.jpg\" data-link=\"/spaceimages/details.php?id=PIA19643\" data-title=\"Spinning Jupiter and Global Map\" id=\"full_image\">\n",
    "\t\t\t\t\tFULL IMAGE\n",
    "\t\t\t\t  </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Near the end of the attributes in the <a /> tag is id=“full_image”. This is significant because in HTML, an id is a \n",
    "# completely unique identifier. \n",
    "\n",
    "# Often, a class is used as an identifier, but only for other HTML tags with similar styling. \n",
    "\n",
    "# For example, when we were scraping the articles, we saw that all of the articles had the same class. \n",
    "# None of the other components of that webpage had that class, though.\n",
    "\n",
    "# An id, on the other hand, can only be used one time throughout the entire page.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Because we want to click the full-size image button, we can go ahead and use the id in our code. \n",
    "\n",
    "# Find and click the full image button\n",
    "full_image_elem = browser.find_by_id('full_image')\n",
    "full_image_elem.click()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# The automated browser should automatically \"click\" the button and change the view to a slideshow of images, so we're\n",
    "# on the right track. We need to click the More Info button to get to the next page. \n",
    "# Let's look at the DevTools again to see what elements we can use for our scraping.\n",
    "\n",
    "there aren't any really unique classes here and no ids at all. This brings us to another useful Splinter functionality: the ability to search for HTML elements by text. In the next available cell, try using Splinter's ability to find elements using text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the more info button and click that\n",
    "browser.is_element_present_by_text('more info', wait_time=1)\n",
    "more_info_elem = browser.links.find_by_partial_text('more info')\n",
    "more_info_elem.click()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First, the code uses the is_element_present_by_text() method to search for an element that has the provided text, in this case “more info.” We've also added an additional argument, wait_time=1. This allows the browser to fully load before we search for the element. Once this line is executed, it will return a Boolean to let us know if the element is present (true) or not (false).\n",
    "\n",
    "Next, we create a new variable, more_info_elem, where we employ the browser.links.find_by_partial_text() method. This method will take our string ‘more info’ to find the link associated with the \"more info\" text.\n",
    "\n",
    "Finally, we tell Splinter to click that link by chaining the .click() function onto our more_info_elem variable.\n",
    "\n",
    "All together, these three lines of code check for the \"more info\" link using only text, store a reference to the link to a variable, then click the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the new page loaded onto our automated browser, it needs to be parsed so we can continue and scrape the \n",
    "# full-size image URL. \n",
    "\n",
    "# Parse the resulting html with soup\n",
    "html = browser.html\n",
    "img_soup = soup(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we need to find the relative image URL. \n",
    "\n",
    "In our browser (make sure you're on the same page as the automated one), activate your DevTools again. This time, let's find the image link for that image. This is a little more tricky. Remember, Robin wants to pull the most recently posted image for her web app. If she uses the image URL below, she'll only ever pull that specific image when using her app.\n",
    "\n",
    "What tags can we use to find the most recent image?\n",
    "\n",
    "The <figure /> and <a /> tags have the image link nested within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/spaceimages/images/largesize/PIA18432_hires.jpg'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'll use all three of these tags (<figure />, <a />, and <img />) to build the URL to the full-size image. \n",
    "\n",
    "# Find the relative image url\n",
    "img_url_rel = img_soup.select_one('figure.lede a img').get(\"src\")\n",
    "img_url_rel"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "figure.lede references the <figure /> tag and its class, lede.\n",
    "\n",
    "a is the next tag nested inside the <figure /> tag.\n",
    "\n",
    "An img tag is also nested within this HTML, so we've included that as well.\n",
    "\n",
    ".get(\"src\") pulls the link to the image.\n",
    "\n",
    "What we've done here is tell BeautifulSoup to look inside the <figure class=”lede” /> tag for an <a /> tag, and then look within that <a /> tag for an <img /> tag. Basically we're saying, \"This is where the image we want lives—use the link that's inside these tags.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We were able to pull the link to the image by pointing BeautifulSoup to where the image will be, instead of grabbing the URL directly. This way, when NASA updates its image page, our code will still pull the most recent image.\n",
    "\n",
    "But if we copy and paste this link into a browser, it won't work. This is because it's only a partial link, as the base URL isn't included. If we look at our address bar in the webpage, we can see the entire URL up there already; we just need to add the first portion to our app.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.jpl.nasa.gov/spaceimages/images/largesize/PIA18432_hires.jpg'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's add the base URL to our code.\n",
    "\n",
    "# img_url = f'https://www.jpl.nasa.gov{img_url_rel}'\n",
    "\n",
    "# This variable holds our f-string.\n",
    "#img_url\n",
    "\n",
    "# This is an f-string, a type of string formatting used for print statements in Python.\n",
    "# f'https://www.jpl.nasa.gov\n",
    "\n",
    "# The curly brackets hold a variable that will be inserted into the f-string when it's executed.\n",
    "# {img_url_rel}\n",
    "\n",
    "\n",
    "# Use the base URL to create an absolute URL\n",
    "img_url = f'https://www.jpl.nasa.gov{img_url_rel}'\n",
    "img_url"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "10.3.5 Scrape Mars Data: Mars Facts\n",
    "\n",
    "We've been able to automate visiting a website to scrape the top news article (title and summary) \n",
    "automated the task of visiting a website and navigating through it to find a full-size image\n",
    "extracted a link based on its location on the page. \n",
    "Our app will always pull the full-size featured image.\n",
    "\n",
    "Nextup is the collection of Mars facts. information is held in a table format.\n",
    "\n",
    "We'll use Devtools extract the table based on its particular tag and attribute pairing set in the HTML code\n",
    "\n",
    "https://space-facts.com/mars\n",
    "\n",
    "All of the data we want is in a <table />\n",
    "Inside the table is <tbody />, which is the body of the table—the headers, columns, and rows.\n",
    "\n",
    "<tr /> is the tag for each table row. Within that tag, the table data is stored in <td /> tags. This is where the columns are established.\n",
    "\n",
    "Instead of scraping each row, or the data in each <td />, we're going to scrape the entire table with Pandas' .read_html() function.\n",
    "\n",
    "add import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>description</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Equatorial Diameter:</th>\n",
       "      <td>6,792 km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Polar Diameter:</th>\n",
       "      <td>6,752 km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mass:</th>\n",
       "      <td>6.39 × 10^23 kg (0.11 Earths)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Moons:</th>\n",
       "      <td>2 (Phobos &amp; Deimos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orbit Distance:</th>\n",
       "      <td>227,943,824 km (1.38 AU)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orbit Period:</th>\n",
       "      <td>687 days (1.9 years)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Surface Temperature:</th>\n",
       "      <td>-87 to -5 °C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>First Record:</th>\n",
       "      <td>2nd millennium BC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Recorded By:</th>\n",
       "      <td>Egyptian astronomers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              value\n",
       "description                                        \n",
       "Equatorial Diameter:                       6,792 km\n",
       "Polar Diameter:                            6,752 km\n",
       "Mass:                 6.39 × 10^23 kg (0.11 Earths)\n",
       "Moons:                          2 (Phobos & Deimos)\n",
       "Orbit Distance:            227,943,824 km (1.38 AU)\n",
       "Orbit Period:                  687 days (1.9 years)\n",
       "Surface Temperature:                   -87 to -5 °C\n",
       "First Record:                     2nd millennium BC\n",
       "Recorded By:                   Egyptian astronomers"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scrape the Facts table \n",
    "\n",
    "# df = pd.read_html('http://space-facts.com/mars/')[0] \n",
    "# With this line, we're creating a new DataFrame from the HTML table. \n",
    "# The Pandas function read_html() specifically searches for and returns a list of tables found in the HTML. \n",
    "# By specifying an index of 0, we're telling Pandas to pull only the first table it encounters, or the first item in \n",
    "# the list. Then, it turns the table into a DataFrame.\n",
    "\n",
    "# df.columns=['description', 'value'] \n",
    "# Here, we assign columns to the new DataFrame for additional clarity.\n",
    "\n",
    "# df.set_index('description', inplace=True) \n",
    "# By using the .set_index() function, we're turning the Description column into the DataFrame's index. \n",
    "# inplace=True means that the updated index will remain in place, without having to reassign the DataFrame to a new \n",
    "# variable.\n",
    "\n",
    "df = pd.read_html('http://space-facts.com/mars/')[0]\n",
    "df.columns=['description', 'value']\n",
    "df.set_index('description', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "How do we add the DataFrame to a web application?\n",
    "Our data is live—if the table is updated, then we want that change to appear in Robin's app also.\n",
    "\n",
    "pandas .to_html() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table border=\"1\" class=\"dataframe\">\\n  <thead>\\n    <tr style=\"text-align: right;\">\\n      <th></th>\\n      <th>value</th>\\n    </tr>\\n    <tr>\\n      <th>description</th>\\n      <th></th>\\n    </tr>\\n  </thead>\\n  <tbody>\\n    <tr>\\n      <th>Equatorial Diameter:</th>\\n      <td>6,792 km</td>\\n    </tr>\\n    <tr>\\n      <th>Polar Diameter:</th>\\n      <td>6,752 km</td>\\n    </tr>\\n    <tr>\\n      <th>Mass:</th>\\n      <td>6.39 × 10^23 kg (0.11 Earths)</td>\\n    </tr>\\n    <tr>\\n      <th>Moons:</th>\\n      <td>2 (Phobos &amp; Deimos)</td>\\n    </tr>\\n    <tr>\\n      <th>Orbit Distance:</th>\\n      <td>227,943,824 km (1.38 AU)</td>\\n    </tr>\\n    <tr>\\n      <th>Orbit Period:</th>\\n      <td>687 days (1.9 years)</td>\\n    </tr>\\n    <tr>\\n      <th>Surface Temperature:</th>\\n      <td>-87 to -5 °C</td>\\n    </tr>\\n    <tr>\\n      <th>First Record:</th>\\n      <td>2nd millennium BC</td>\\n    </tr>\\n    <tr>\\n      <th>Recorded By:</th>\\n      <td>Egyptian astronomers</td>\\n    </tr>\\n  </tbody>\\n</table>'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_html()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See \"Mission_to_Mars.py\" for striped down version of this for reference for module challenge "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
