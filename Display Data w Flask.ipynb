{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "FOr our app.py file\n",
    "\n",
    "from flask import Flask, render_template\n",
    "from flask_pymongo import PyMongo\n",
    "import scraping\n",
    "\n",
    "The first line says that we'll use Flask to render a template.\n",
    "The second line says we'll use PyMongo to interact with our Mongo database.\n",
    "Finally, the last line says that to use the scraping code, we will convert from Jupyter notebook to Python."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Under these lines, let's add the following to set up Flask:\n",
    "\n",
    "app = Flask(__name__)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We also need to tell Python how to connect to Mongo using PyMongo. Next, add the following lines:\n",
    "\n",
    "# Use flask_pymongo to set up mongo connection\n",
    "app.config[\"MONGO_URI\"] = \"mongodb://localhost:27017/mars_app\"\n",
    "mongo = PyMongo(app)\n",
    "\n",
    "app.config[\"MONGO_URI\"] tells Python that our app will connect to Mongo using a URI, a uniform resource identifier similar to a URL.\n",
    "\n",
    "\"mongodb://localhost:27017/mars_app\" is the URI we'll be using to connect our app to Mongo. This URI is saying that the app can reach Mongo through our localhost server, using port 27017, using a database named \"mars_app\"."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Set Up App Routes\n",
    "\n",
    "The code we create next will set up our Flask routes: one for the main HTML page everyone will view when visiting the web app, and one to actually scrape new data using the code we've written.\n",
    "\n",
    "Flask routes bind URLs to functions. For example, the URL \"ourpage.com/\" brings us to the homepage of our web app. The URL \"ourpage.com/scrape\" will activate our scraping code.\n",
    "\n",
    "These routes can be embedded into our web app and accessed via links or buttons."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First, let's define the route for the HTML page. In our script, type the following:\n",
    "\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "   mars = mongo.db.mars.find_one()\n",
    "   return render_template(\"index.html\", mars=mars)\n",
    "   \n",
    "This route, @app.route(\"/\"), tells Flask what to display when we're looking at the home page, index.html (index.html is the default HTML file that we'll use to display the content we've scraped). This means that when we visit our web app's HTML page, we will see the home page.\n",
    "\n",
    "Within the def index(): function the following is accomplished:\n",
    "\n",
    "mars = mongo.db.mars.find_one() uses PyMongo to find the \"mars\" collection in our database, which we will create when we convert our Jupyter scraping code to Python Script. We will also assign that path to the mars variable for use later.\n",
    "\n",
    "return render_template(\"index.html\" tells Flask to return an HTML template using an index.html file. We'll create this file after we build the Flask routes.\n",
    "\n",
    ", mars=mars) tells Python to use the \"mars\" collection in MongoDB.\n",
    "\n",
    "This function is what links our visual representation of our work, our web app, to the code that powers it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Our next function will set up our scraping route. This route will be the \"button\" of the web application, the one that will scrape updated data when we tell it to from the homepage of our web app. It'll be tied to a button that will run the code when it's clicked.\n",
    "\n",
    "Let's add the next route and function to our code. In the editor, type the following:\n",
    "\n",
    "@app.route(\"/scrape\")\n",
    "def scrape():\n",
    "   mars = mongo.db.mars\n",
    "   mars_data = scraping.scrape_all()\n",
    "   mars.update({}, mars_data, upsert=True)\n",
    "   return \"Scraping Successful!\"\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "The first line, @app.route(“/scrape”) defines the route that Flask will be using. This route, “/scrape”, will run the function that we create just beneath it.\n",
    "\n",
    "The next lines allow us to access the database, scrape new data using our scraping.py script, update the database, and return a message when successful. Let's break it down.\n",
    "\n",
    "First, we define it with def scrape():.\n",
    "\n",
    "Then, we assign a new variable that points to our Mongo database: mars = mongo.db.mars.\n",
    "\n",
    "Next, we created a new variable to hold the newly scraped data: mars_data = scraping.scrape_all(). In this line, we're referencing the scrape_all function in the scraping.py file exported from Jupyter Notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we've gathered new data, we need to update the database using .update(). Let's take a look at the syntax we'll use, as shown below:\n",
    "\n",
    ".update(query_parameter, data, options)\n",
    "\n",
    "We're inserting data, so first we'll need to add an empty JSON object with {} in place of the query_parameter. \n",
    "\n",
    "Next, we'll use the data we have stored in mars_data. \n",
    "\n",
    "Finally, the option we'll include is upsert=True. This indicates to Mongo to create a new document if one doesn't already exist, and new data will always be saved (even if we haven't already created a document for it).\n",
    "\n",
    "The entire line of code looks like this: mars.update({}, mars_data, upsert=True).\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Finally, we will add a message to let us know that the scraping was successful: return \"Scraping successful!\"."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The final bit of code we need for Flask is to tell it to run. Add these two lines to the bottom of your script and save your work:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   app.run()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "10.5.2 Update the Code\n",
    "\n",
    "Before her code is ready for deployment, she'll need to integrate her scraping code in a way that Flask can handle. That means updating it to include functions and even some error handling. This will help with our app's performance and add a level of professionalism to the end product."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We've already downloaded our Jupyter Notebook code and converted it to a Python script, but it's not quite ready to be used as part of our Flask app yet.\n",
    "\n",
    "There are two big things we want to update in our code: we want to refactor it to include functions, and we will be adding some error handling into the mix.\n",
    "\n",
    "Functions are a very necessary part of programming. They allow developers to create code that will be reused as needed, instead of needing to rewrite the same code repeatedly.\n",
    "\n",
    "In our case, we want our code to be reused, and often, to pull the most recent data. \n",
    "\n",
    "Functions enable this capability by bundling our code into something that is easy for us (and once it's deployed, whoever else we share the web app with) to use and reuse as needed.\n",
    "\n",
    "We need to do this for our \"scraping.py\" script. First up..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "News Title and Paragraph \n",
    "\n",
    "In our \"Scraping.py\" file, the beginning of our first function is:\n",
    "# Visit the mars nasa news site\n",
    "\n",
    "We'll call this function mars_news. Begin the function by defining it, then indent the code as needed to adhere to function syntax. It should look like the code below:\n",
    "\n",
    "def mars_news():\n",
    "\n",
    "   # Visit the mars nasa news site\n",
    "   url = 'https://mars.nasa.gov/news/'\n",
    "   browser.visit(url)\n",
    "   \n",
    "   ....etc.\n",
    "   \n",
    "To complete the function, we need to add a return statement so python know that the function is complete. \n",
    "\n",
    "Instead of having our title and paragraph printed within the function, we want to return them from the function so they can be used outside of it. We'll adjust our code to do so by deleting news_title and news_p and include them in the return statement instead:\n",
    "\n",
    "   # Use the parent element to find the paragraph text\n",
    "   news_p = slide_elem.find('div', class_=\"article_teaser_body\").get_text()\n",
    "\n",
    "   return news_title, news_p"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "add an argument to the function.\n",
    "\n",
    "Update your function like this:\n",
    "\n",
    "def mars_news(browser):\n",
    "\n",
    "we're telling Python that we'll be using the browser variable we defined outside the function. All of our scraping code utilizes an automated browser, and without this section, our function wouldn't work."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cThe finishing touch is to add error handling to the mix.\n",
    "the most common cause of an error is when the webpage's format has changed and the scraping code no longer matches the new HTML elements, so we'll use a \"try and except\" clause before the scraping code. \n",
    "\n",
    "# Add try/except for error handling\n",
    "    try:\n",
    "        slide_elem = news_soup.select_one(\"ul.item_list li.slide\")\n",
    "        # Use the parent element to find the first 'a' tag and save it as 'news_title'\n",
    "        news_title = slide_elem.find(\"div\", class_=\"content_title\").get_text()\n",
    "        # Use the parent element to find the paragraph text\n",
    "        news_p = slide_elem.find(\"div\", class_=\"article_teaser_body\").get_text()\n",
    "    except AttributeError:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Let's update our featured image the same way.\n",
    "\n",
    "Featured Image Function\n",
    "\n",
    "1. Declare and define our function.\n",
    "def featured_image(browser):\n",
    "\n",
    "2. Remove print statement(s) and return them instead.\n",
    "    In our Jupyter Notebook version of the code, we printed the results of our scraping by simply stating the variable (e.g., after assigning data to the img_url variable, we simply put img_url on the next line to view the data). We still want to view the data output in our Python script, but we want to see it at the end of our function instead of within it.\n",
    "\n",
    "return img_url\n",
    "\n",
    "3. Add error handling for AttributeError.\n",
    "\n",
    "try:\n",
    "   # find the relative image url\n",
    "   img_url_rel = img_soup.select_one('figure.lede a img').get(\"src\")\n",
    "\n",
    "except AttributeError:\n",
    "   return None"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Mars Facts Function\n",
    "Code for the facts table will be updated in a similar manner to the other two. This time, though, we'll be adding BaseException to our except block for error handling.\n",
    "\n",
    "A BaseException is a little bit of a catchall when it comes to error handling. It is raised when any of the built-in exceptions are encountered and it won't handle any user-defined exceptions. We're using it here because we're using Pandas' read_html() function to pull data, instead of scraping with BeautifulSoup and Splinter. The data is returned a little differently and can result in errors other than AttributeErrors, which is what we've been addressing so far.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "10.5.3 Integrate MongoDB Into the Web App\n",
    "\n",
    "Before we make our website look pretty (you never know when NASA is looking for its new analyst), we need to connect to Mongo and establish communication between our code and the database we're using. We'll add this last bit of code to our scraping.py script.\n",
    "\n",
    "connect to Mongo and establish communication between our code and the database we're using. We'll add this last bit of code to our scraping.py script.\n",
    "\n",
    "At the top of our scraping.py script, just after importing the dependencies, we'll add one more function. This function differs from the others in that it will:\n",
    "\n",
    "Initialize the browser.\n",
    "1. Create a data dictionary.\n",
    "2. End the WebDriver and return the scraped data.\n",
    "3. Let's define this function as \"scrape_all\" and then initiate the browser.\n",
    "\n",
    "def scrape_all():\n",
    "   # Initiate headless driver for deployment\n",
    "   browser = Browser(\"chrome\", executable_path=\"chromedriver\", headless=True)\n",
    "   \n",
    "While we can see the word \"browser\" here twice, one is the name of the variable passed into the function and the other is the name of a parameter. Coding guidelines do not require that these match, even though they do in our current code.\n",
    "\n",
    "When we were testing our code in Jupyter, headless was set as False so we could see the scraping in action. Now that we are deploying our code into a usable web app, we don't need to watch the script work (though it's totally okay if you still want to).\n",
    "\n",
    "When scraping, the \"headless\" browsing session is when a browser is run without the users seeing it at all. So, when headless=True is declared as we initiate the browser, we are telling it to run in headless mode. All of the scraping will still be accomplished, but behind the scenes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Next, we're going to set our news title and paragraph variables (remember, this function will return two values).\n",
    "\n",
    "   news_title, news_paragraph = mars_news(browser)\n",
    "   \n",
    "This line of code tells Python that we'll be using our mars_news function to pull this data."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now that we have our browser ready for work, we need to create the data dictionary. Add the following code to our scrape_all() function:\n",
    "\n",
    "# Run all scraping functions and store results in dictionary\n",
    "data = {\n",
    "      \"news_title\": news_title,\n",
    "      \"news_paragraph\": news_paragraph,\n",
    "      \"featured_image\": featured_image(browser),\n",
    "      \"facts\": mars_facts(),\n",
    "      \"last_modified\": dt.datetime.now()\n",
    "}\n",
    "\n",
    "This dictionary does two things: It runs all of the functions we've created—featured_image(browser), for example—and it also stores all of the results. When we create the HTML template, we'll create paths to the dictionary's values, which lets us present our data on our template. We're also adding the date the code was run last by adding \"last_modified\": dt.datetime.now(). For this line to work correctly, we'll also need to add import datetime as dt to our imported dependencies at the beginning of our code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "To finish up the function, there are two more things to do. The first is to end the WebDriver using the line browser.quit(). You can quit the automated browser by physically closing it, but there's a chance it won't fully quit in the background. By using code to exit the browser, you'll know that all of the processes have been stopped.\n",
    "\n",
    "Second, the return statement needs to be added. This is the final line that will signal that the function is complete, and it will be inserted directly beneath browser.quit(). We want to return the data dictionary created earlier, so our return statement will simply read return data.\n",
    "\n",
    "   # Stop webdriver and return data\n",
    "   browser.quit()\n",
    "   return data\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The last step we need to add is similar to the last code block in our app.py file. At the bottom of our scraping.py script, add the following:\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # If running as script, print scraped data\n",
    "    print(scrape_all())\n",
    "    \n",
    "This last block of code tells Flask that our script is complete and ready for action. The print statement will print out the results of our scraping to our terminal after executing the code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PythonData",
   "language": "python",
   "name": "pythondata"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
